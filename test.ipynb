{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ë™ì ì¸ ì´ì•¼ê¸°ë“¤ì´ ë§ì´ ë‚˜ì˜¤ëŠ”ë°ìš”.\n",
      "ì´ëŸ° ê²ƒë“¤ì´ ì¢€ ë” ë§ì€ ë¶„ë“¤ì—ê²Œ ì „ë‹¬ë  ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤.\n",
      "ë„¤. ì˜¤ëŠ˜ ë§ì”€ ê°ì‚¬í•©ë‹ˆë‹¤.\n",
      "ì˜ˆ. ê³ ë§™ìŠµë‹ˆë‹¤.</d> ì´ì‹­ì¼ ì„¸ê¸° í•œêµ­ í˜„ëŒ€ë¯¸ìˆ ì„ ëŒ€í‘œí•˜ëŠ” ì‘ê°€ ë°±ë‚¨ì¤€ ì”¨ê°€ ì‹­êµ¬ì„¸ê¸°ë¥¼ í’ë¯¸í•œ ê±°ì¥ë“¤ì˜ ì‘í’ˆì„ ëª¨ì•„ë†“ì€ ì „ì‹œíšŒë¥¼ ì—´ì—ˆì£ .\n",
      "ë°± ì”¨ì˜ ì‘í’ˆ ì„¸ê³„ë¥¼ ì¡°ëª…í•´ë³´ëŠ” ì‹œê°„ì…ë‹ˆë‹¤.\n",
      "ê¹€ì§€ì„  ê¸°ìê°€ ì†Œê°œí–ˆìœ¼ë‹ˆê¹Œ ë„¤~ ë¨¼ì € ê·¸ ì´ì²œíŒ”ë…„ë¶€í„° ì²œ ë…„ê¹Œì§€ ìš°ë¦¬ ë¯¸ìˆ ê³„ë¥¼ ì´ëŒì–´ì˜¨ ì›ë¡œì‘ê°€ë“¤ì„ ë§Œë‚˜ë³´ê² ë‹¤.\n",
      "ì•„ë‹ˆë©´ ì§€ê¸ˆ í˜„ì¬ë¡œ ì¹˜ìë©´ì€ ë­ í•œ ì‹œëŒ€ë¥¼ ì´ëŒì—ˆë˜ ê±°ëŠ” ëˆ„êµ¬ëƒ? ì•„í•˜ ì˜ˆ ê·¸ëŸ°\n"
     ]
    }
   ],
   "source": [
    "# KoGPT2_TEST\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")\n",
    "['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = 'ê°ë™ì ì¸ ì´ì•¼ê¸°'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids,\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('readvice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82e657652c1f559b98fb141e76bcce2ec0f3958c3a6000b4409466ee456e5f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
